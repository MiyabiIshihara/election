---
title: "Multilevel Model Project"
author: "chuan-Ya Hsu, Dana Weisenfield, Diana Ma, Miyabi Ishihara"
date: "December 2016"
output: html_document
---

**Objective**
The aim of this project is to predict the results of the 2016 United States presidential election using Bayesian modeling. The framework for our analysis is the election prediction model used by Nate Silver’s FiveThirtyEight.com. R and Stan were used to implement the Bayesian model and visually display the results. Bayesian modelling is appropriate for predicting election results because it can account for the uncertainty in polls and  ...

**Data**
The data come from three different sources. Polling data and results from past elections were obtained from data compiled by Marc Scott for Multi-level Models: Growth Curves. State-level demographic variables came from the Census. 
WHERE DID EDUCATION VARIABLES COME FROM?

- Date
- pctRep08: percentage of Republicans in 2008
- pctRep12: percentage of Republicans in 2012
- Percentage of people who received high school diploma 
- Percentage of people who received bachelor’s degree
- Percentage of Blacks or African Americans
- Percentage of Asians, American Indian, Native Hawaiian and other Pacific Islanders, or those of two+ races
- Percentage of Hispanics
- Percentage of people age < 35 
- Percentage of female 


**Methodology**

We used the following packages for our analysis
```{r load-packages, message=FALSE, warning=FALSE, echo = FALSE}
library(rstan)
library(lme4)
library(foreign)
library(nlme)
library(lmtest)
library(plyr)
library(maps)         
library(mapproj)
library(RColorBrewer)
```

Prior to fitting a Bayesian model in Stan, we first fit a non-Bayesian multi-level model using the lmer package in R. 
Results from lmer were used to verify that the results from the Bayesian models were reasonable. 


```{r results='hide', message=FALSE, warning=FALSE}
setwd("/Users/dlweisenfeld/Documents/GrowthCurvesPracticum")

data2 <- read.dta("Election2016.dta") 
#data <- read.dta("votesRepubThru2012.dta")
edu <- read.csv("education_level_state.csv")
census <- read.csv("census_data.csv")
unique(data2$pctRep12)

# merging edu and data2
edu$state <- tolower(edu$state)
data2 <- merge(data2, edu, by = "state")
data2 <- rename(data2, c("highschool_higher_percent" = "HS", "bachelor_higher_percent" = "BS"))
head(data2)

# merging census and data2
head(census)
census <- rename(census, c("STATE" = "state"))
census$state <- tolower(census$state) 
data2 <- merge(data2, census, by = "state")

head(data2)
```


**0. Unconditional Model**

We first fit an unconditional model with varying intercepts. Let $i$ indicate poll and $j$ indicate state, then percent Democrat $Y_{ij}$ is modeled as: 
$$
Y_{ij} = b_0 + \zeta_{0j} + \epsilon_{ij},
$$
where $\zeta_{0j} \sim N(0,\sigma_{\zeta_0}^2)$ and $\epsilon_{ij} \sim N(0, \sigma_{\epsilon}^2)$, independently of one another. ICC is 0.915, meaning that $91.5 \%$ of the variance is explained by between states. This indicates that accounting for the hierarchical structure of the data is necessary. 


```{r results='hide', message=FALSE, warning=FALSE}
## Step 1. Run unconditional model
head(data2)
ucm <- lmer(DemPctHead2Head ~ (1|id), data = data2)
summary(ucm)
0.008227/(0.008227+0.000762)
```

**1. Model with Covariates and Varying Slopes**

We now fit the following model: 
$$
Y_{ij} = b_0 + b_1(pctRep8_{ij}) + b_2(pctRep12_{ij}) + b_3(Time_{ij}) + \zeta_{ij}(Time_{ij}) + \zeta_{0j} + \epsilon_{ij}
$$

We include three covariates -- percent Republican in the years 2008 and 2012, and time of the poll. Time represents the number of days before the actual election, but converted to unit in years. Note that Time is negative and takes 0 on the day of the election. 

We allow slopes of Time to vary by state. This means that we consider the effect of time on percent Democrat to differ by state -- some states may be very stable and always support one party over another, while other states may be more variable. 

Our estimated parameters are: 
$$
\hat{Y}_{ij} = 0.5 - 0.00066(pctRep8_{ij}) - 0.0078(pctRep12_{ij}) - 0.013(Time_{ij}),
$$
where $\hat{\sigma}_{\zeta_{1j}}^2 = 0.0015$, $\hat{\sigma}_{\zeta_{0j}}^2 = 0.0008$, and $\hat{\epsilon}_{ij} = 0.0007$. It appears that percent Republican in 2012 is significantly and negatively associated with percent Democrat in 2016. 

```{r results='hide', message=FALSE, warning=FALSE}
head(data2)
## random intercept, random slopes, covariates
fit.cov1 <- lmer(DemPctHead2Head ~ I(date/365) + pctRep08 + pctRep12 + (I(date/365) | id), data = data2, subset = date > -250) 
summary(fit.cov1)

## fitting the same model in lme to get p-values
summary(lme(DemPctHead2Head ~ I(date/365) + pctRep08 + pctRep12, random = ~ I(date/365) | id, data = data2, subset = date > -250)) 

```

**2. Quadratic Model**

We add a quadratic growth to the model: 
$$
Y_{ij} = b_0 + b_1(pctRep8_{ij}) + b_2(pctRep12_{ij}) + b_3(Time_{ij}) + b_4(Time_{ij}^2) + \zeta_{3j}(Time_{ij}) + \zeta_{0j} + \epsilon_{ij}
$$

Estimated parameters are:
$$
\hat{Y}_{ij} = 0.5 - 0.00075(pctRep8_{ij}) - 0.0077(pctRep12_{ij}) + 0.038(Time_{ij}) + 0.093(Time_{ij}^2),
$$
where variance of random effects remain approximately the same as before (only slightly decreased): $\hat{\sigma}_{\zeta_{1j}}^2 = 0.0015$, $\hat{\sigma}_{\zeta_{0j}}^2 = 0.0008$, and $\hat{\epsilon}_{ij} = 0.0007$. It appears that now, pctRep12 as well as $Time^2$ are significant ($p<0.05$), and Time is somehwat significant ($p<0.1$). 

```{r results='hide', message=FALSE, warning=FALSE}
## adding quadratic growth 
fit.cov2 <- lmer(DemPctHead2Head ~ I(date/365) + I((date/365)^2) + pctRep08 + pctRep12 + (I(date/365) | id), data = data2, subset = date > -250) 
summary(fit.cov2)

summary(lme(DemPctHead2Head ~ I(date/365) + I((date/365)^2) + pctRep08 + pctRep12, random = ~ I(date/365) | id, data = data2, subset = date > -250))
```

**3. Quadratic Model with Random Slopes, without $Time^2$**
$$
Y_{ij} = b_0 + b_1(pctRep8_{ij}) + b_2(pctRep12_{ij}) + b_3(Time_{ij}) + \zeta_{3j}(Time_{ij}) + \zeta_{4j}(Time_{ij}^2) + \zeta_{0j} + \epsilon_{ij}
$$

**4. Quadratic Model with Random Slopes, with $Time^2$**
$$
Y_{ij} = b_0 + b_1(pctRep8_{ij}) + b_2(pctRep12_{ij}) + b_3(Time_{ij}) + b_4(Time_{ij}^2) + \zeta_{3j}(Time_{ij}) + \zeta_{4j}(Time_{ij}^2) + \zeta_{0j} + \epsilon_{ij}
$$

```{r results='hide', message=FALSE, warning=FALSE}
## model 3. adding quadratic growth and new random slopes
fit.cov3 <- lmer(DemPctHead2Head ~ I(date/365)  + pctRep08 + pctRep12 + (I(date/365) + I((date/365)^2) | id), data = data2, subset = date > -250) 
summary(fit.cov3)


## model 4. 
fit.cov4 <- lmer(DemPctHead2Head ~ I(date/365) + I((date/365)^2) + pctRep08 + pctRep12 + (I(date/365) + I((date/365)^2) | id), data = data2, subset = date > -250) 
```


**Model Comparison and Selection**

It appears that model 3 (quadratic growth and two random slopes without fixed $Time^2$ variable) fit well among the models we have tried. 
```{r results='hide', message=FALSE, warning=FALSE}
## likelihood ratio test
lrtest(fit.cov1, fit.cov2) # reject null
lrtest(fit.cov1, fit.cov3) # reject null
lrtest(fit.cov2, fit.cov3) # reject null
lrtest(fit.cov3, fit.cov4) # fail to reject. suggests that we go with model 3. 
extractAIC(fit.cov1)
extractAIC(fit.cov2)
extractAIC(fit.cov3)
extractAIC(fit.cov4) 
```


The simplest model Bayesian model we looked at included random intercepts for each state as well as three predictors--date of the poll and percent of the state that voted Republican in both 2008 and 2012. This simple model overestimated the share of the Democratic vote; even solidly red states such as Indiana were predicted to go blue in 2016. 

After successfully fitting a random intercept model, random slopes were added. In comparison to the random intercept model, adding random slopes slightly improved predictions. 
- explain why random slopes are theoretically important for this model
- include mathematical notation for random slope
- results from random slope model

However, the model with random slopes and random intercepts still overpredicted Democratic percentages. To try and improve predictions we added state-level predictors to the model. Since education was found to be such a big determinant of election behavior, we surmised that education variables would improve state predictions. In addition to variables containing the proportion of state residents with a high school and college education, variables on the age, gender, and racial makeup of the state were also added. Specifically, variables containing the proportion of black citizens, the proportion of all other minority citizens, the proportion of citizens under 35, the proportion of females, and the proportion of citizens of Hispanic origin were added to the model. However, adding these variables resulted in convergence problems. 

The model output showed that the intercept and the coefficient on high school education had very small effective sample sizes. An examination of the trace plots also showed extremely poor convergence on these variables. Additional diagnostics showed that the intercept was highly correlated with high school education. However, removing the high school education variable from the model did not improve convergence. After removing high school education the intercept was highly correlated with college education and the trace plots still indicated poor convergence. This occurred even after imposing independence on the random intercept and random slope. Other combinations of covariates that did not include either education variable also resulted in convergence problems. No matter what variables were included, the intercept always ended up being correlated with some covariate and having a very small effective sample size. Doubling the number of iterations from 2,000 to 4,000 did not improve convergence.  

Finally, an attempt was made to fix alpha to be zero add the intercept term in to theta directly. While this improved convergence, the intercept and coefficient were then negative, which does not make sense. Since our attempts to add covariates were unsuccessfully, we are reporting the results from the random intercept, random slope model with three covariates. 

**Stan**
```{r engine='cat', engine.opts=list(file = "mod0.stan", lang = "stan")}

stanMLM <- "data {
    int<lower=0> N;   // number of obs
    int<lower=1> K;   // number of predictors
    int<lower=0> M;   // number of groups (states)

    matrix[N,K] x; 
    vector[N] y;        // outcomes
    int<lower=1> id[N]; // unique group ID

    int<lower=0> N_new;      // number of predictions (one for each state)
    matrix[N_new, K] x_new;  //covariate matrix for predictions
}

parameters {
    vector[K] beta;           // coefficients
    real<lower=0> omega_alpha0;
    real<lower=0> omega_alphaT;
    real<lower=-1, upper=1> cor_alpha0T; 
    
    real<lower=0> sig_eps;    // y eq: variation of error
    vector[2] alpha[M];       //random intercept and slope //
    vector[N_new] y_new;      // predictions for each state
}


transformed parameters { 
    vector[N] theta;          // mean pred line
    vector[N_new] theta_new;  

    matrix[2,2] sigma;
    vector[2] mu;
    sigma[1,1] <- pow(omega_alpha0,2);
    sigma[2,2] <- pow(omega_alphaT,2);
    sigma[1,2] <- cor_alpha0T * omega_alpha0 * omega_alphaT;
    sigma[2,1] <- cor_alpha0T * omega_alpha0 * omega_alphaT;

    mu[1] <- beta[1];
    mu[2] <- beta[2];

    for (i in 1:N){
    theta[i] <- alpha[id[i]][1] + alpha[id[i]][2]*x[i,2] + beta[3]*x[i,3] + beta[4]*x[i,4] + beta[5]*x[i,5] + beta[6]*x[i,6] + beta[7]*x[i,7] + beta[8]*x[i,8] + beta[9]*x[i,9] + beta[10]*x[i,10] + beta[11]*x[i,11];
    }

    for (i in 1:N_new){
    theta_new[i] <- alpha[id[i]][1] + alpha[id[i]][2]*0 + beta[2]*0 + beta[3]*x_new[i,3] + beta[4]*x_new[i,4] + beta[5]*x_new[i,5] + beta[6]*x_new[i,6] + beta[7]*x_new[i,7] + beta[8]*x_new[i,8] + beta[9]*x_new[i,9] + beta[10]*x_new[i,10] + beta[11]*x_new[i,11];
    }
}


model {
    omega_alpha0 ~ cauchy(0,5);
    omega_alphaT ~ cauchy(0,5); 
    sig_eps ~ cauchy(0,5); //same

    y ~ normal(theta, sig_eps);  // simple normal model for outcome
    y_new ~ normal(theta_new, sig_eps); 

    for (i in 1:M){
       alpha[i] ~ multi_normal(mu, sigma);
    }
}"
```


```{r, echo=FALSE, eval=FALSE}
set.seed(12345)
M <- length(unique(data2$id)); M 
N <- nrow(data2); N
x <- model.matrix( ~ 1 + date + pctRep12 + pctRep08 + HS + BS + black_prop + asian_prop + age_prop + sex_prop + origin_prop, data2)
K <- ncol(x); K

# prediction matrix
z1 <- tapply(data2$pctRep08, data2$id, "[", 1); z1
z2 <- tapply(data2$pctRep12, data2$id, "[", 1); z2
z3 <- tapply(data2$HS, data2$id, "[", 1); z3
z4 <- tapply(data2$BS, data2$id, "[", 1); z4
z5 <- tapply(data2$black_prop, data2$id, "[", 1); z5
z6 <- tapply(data2$BS, data2$asian_prop, "[", 1); z6
z7 <- tapply(data2$HS, data2$age_prop, "[", 1); z7
z8 <- tapply(data2$BS, data2$sex_prop, "[", 1); z8
z9 <- tapply(data2$BS, data2$origin_prop, "[", 1); z9
x_new <- model.matrix(~ 1 + rep(0,50) + z1 + z2 + z3 + z4 + z5 + z6 + z7 + z8 + z9); x_new
N_new <- length(z1); N_new # number of predictions to make
y <- data2$DemPctHead2Head; length(y)
id <- data2$id

dat1 <- list(y=y, x=x, x_new = x_new, N_new = N_new, M=M, N=N, K=K, id=id)
#dat1

#fit_stanMLM <- stan(model_code = stanMLM, fit = NULL, model_name = "MLM", data = dat1, iter = 2000, chains = 3, sample_file = 'MLM.csv', verbose = TRUE)

#traceplot(fit_stanMLM, inc_warmup = TRUE)
fit_stanMLM <- readRDS("results_4covars")
print(fit_stanMLM, pars=c("beta", "y_new", "sig_eps", "sigma"), digits = 6) 
#saveRDS(fit_stanMLM, "fit_stanMLM_edu.rds")
#b <- readRDS("fit_stanMLM.rds")
```

```{r stan, echo=FALSE}
fit_stanMLM <- readRDS("results1208.Rds")
print(fit_stanMLM, pars=c("beta", "y_new", "sig_eps", "sigma")) 
```


Sanity Check
```{r, echo=FALSE}
# sanity check
summary(lmer(DemPctHead2Head ~ date + pctRep12 + pctRep08 + HS + BS + (date |id), data = data2))

summary(lmer(DemPctHead2Head ~ date + pctRep12 + pctRep08 + HS + BS + black_prop + asian_prop + age_prop + sex_prop + origin_prop + (date |id), data = data2))

summary(lme(DemPctHead2Head ~ date + pctRep12 + pctRep08 + HS + BS + black_prop + asian_prop + age_prop + sex_prop + origin_prop, random = ~ date|id, data = data2))


## miscellaneous
print(fit_stanMLM, pars = "y_new")           # printing state by state predictions
fit.sims <- extract(fit_stanMLM, permuted=TRUE)
```




Graphing State Results
```{r, eval=FALSE, echo=FALSE}

state.preds <- colMeans(fit.sims$y_new)    # extracting vector of state predictions
state.by.id <- tapply(data2$state, data2$id, "[", 1)  # linking states with their id numbers
preds <- cbind.data.frame("state"=state.by.id,"pctDem"=state.preds)
preds <- preds[order(preds$pctDem), ]
preds$order_state <- seq(1,50,1)
rownames(preds) = 1:50


new_y <- extract(fit_stanMLM, pars = "y_new")
pred <- apply(new_y[[1]], 2, quantile, probs = c(0.025, 0.5, 0.975))
pred <- t(pred)
colnames(pred) <- c("lower", "center", "upper")
pred <- as.data.frame(pred)
pred <- pred[order(pred$center), ]
pred$order_state <- seq(1,50,1)
rownames(pred) = 1:50
pred

preds <- merge(pred, preds, by = "order_state")
preds



## better plot


p <- ggplot(data = preds, aes(x = center, y = reorder(state, center))) 
p <- p + geom_errorbarh(aes(xmin = lower, xmax = upper), colour = 'gray80')
p <- p + geom_point(aes(x = center, y = reorder(state, center), color = center))
p <- p + xlab("Pr(Clinton wins)") + ylab("")
p <- p +  guides(color = FALSE)
p <- p + scale_color_gradientn(colours = c("red", "blue"))
p <- p + geom_vline(xintercept = 0.5, colour = "gray90", aes(size = 1))
p <- p + theme(panel.background = element_rect(fill = 'gray97', colour = 'white'))
p 


## simple plot
plot(y = row.names(preds[(preds[,2]>=.5),1]), x = preds[(preds[,2]>=.5),2], col="blue", xlim=c(.25, .75), xlab="Percent Democrat", ylab="State")
points(y=row.names(preds[(preds[,2]<.5),1]), x=preds[(preds[,2]<.5),2], col="red")
abline(v=0.5)


plot(fit_stanMLM , pars = c("y_new", "sig_eps"))

```



Map Plot
```{r, eval=FALSE, echo=FALSE}
# MAP PLOT #
data(state)           # making state map file available 
# extracting names of regions in the map
mapnames = map("state", plot=FALSE)$names 
#mapnames


# disconnected regions (i.e. long island) are listed seperately from the rest of the state
# removing the region so that only the actual state is listed
mapnames.state = ifelse(regexpr(":",mapnames) < 0,mapnames, substr(mapnames, 1, regexpr(":",mapnames)-1))


# linking colors to state predictions
red.blue <- colorspace::diverge_hsv(4)

predsRD = ifelse(preds[,2] < .5, "red", "blue") # dichotomizing prediction results
repubs <- which(preds[,2] < .5)
dems <- which(preds[,2] >= .5)

predsRD[repubs] = ifelse(preds[preds[,2] < .5, 2] > .48, red.blue[3], red.blue[4])
predsRD[dems] = ifelse(preds[preds[,2] > .5, 2] < .52, red.blue[2], red.blue[1])

cols = predsRD[match(mapnames.state,preds[,1])] 
map("state", fill=TRUE, col=cols, proj="albers", param=c(35,50))
```


```{r, echo=FALSE}
# ANSWERING QUESTIONS 
state.by.id <- tapply(data2$state, data2$id, "[", 1)  # linking states with their id numbers
state_dist <- as.data.frame(fit.sims$y_new) # 
names(state_dist) <- as.vector(state.by.id)

state_probs <- c()
for (i in 1:50) {
  state_probs[i] <- length(state_dist[,i][state_dist[,i] > .5])/length(state_dist[,i])
} 

# Key swing states in 2016:
# Florida, Pennsylvania, Ohio, Michigan, Nevada, New Hampshire, North Carolina, Virginia, and Wisconsin

# Probability that Wisconsin, Ohio, and Michigan all go Democrat?
q2 <- nrow(state_dist[(state_dist$wisconsin > .5) & (state_dist$ohio > .5) & (state_dist$michigan > .5),
                          c("wisconsin", "ohio", "michigan")])/nrow(state_dist)

# Probability that Florida, Virginia, and Pennsylvania all go Democrat?
q2 <- nrow(state_dist[(state_dist$florida > .5) & (state_dist$pennsylvania > .5) & (state_dist$virginia > .5),])/nrow(state_dist)
```
